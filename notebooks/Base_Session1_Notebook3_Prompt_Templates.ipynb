{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV62MgB31wR1"
   },
   "source": [
    "# Session 1.3: Prompt Engineering and Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HASMrHUQAD06"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1h5mUoP7Sgit2ZW6bWRQV5qiDWpiBnRj1?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SYRxyemAEEr"
   },
   "source": [
    "## Overview\n",
    "\n",
    "Prompt engineering is the art of designing effective instructions for LLMs. In this notebook, you'll learn:\n",
    "\n",
    "- **Prompt basics** and best practices\n",
    "- **Prompt templates** in LangChain\n",
    "- **Few-shot prompting**\n",
    "- **Output parsers** for structured responses\n",
    "- **Prompt composition** and chaining\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "‚úÖ Master prompt engineering techniques  \n",
    "‚úÖ Create reusable prompt templates  \n",
    "‚úÖ Use few-shot examples effectively  \n",
    "‚úÖ Parse and structure LLM outputs  \n",
    "‚úÖ Compose complex prompts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16572,
     "status": "ok",
     "timestamp": 1761237394091,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "J7F9_LzZ7VYI",
    "outputId": "6eec05f6-11b5-4167-ce8f-81c5f2dbb1b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/76.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.5/2.5 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/64.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-openai langchain-core langchain-community\n",
    "!pip install -q python-dotenv pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhyS2kWJ6tJR"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LXzsWuS1wR4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Set OpenAI API key from Google Colab's user environment or default\n",
    "def set_openai_api_key(default_key: str = \"YOUR_API_KEY\") -> None:\n",
    "    \"\"\"Set the OpenAI API key from Google Colab's user environment or use a default value.\"\"\"\n",
    "    #if not (userdata.get(\"OPENAI_API_KEY\") or \"OPENAI_API_KEY\" in os.environ):\n",
    "    try:\n",
    "      os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"MDX_OPENAI_API_KEY\")\n",
    "    except:\n",
    "      os.environ[\"OPENAI_API_KEY\"] = default_key\n",
    "\n",
    "set_openai_api_key()\n",
    "\n",
    "# Initialize LLM\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "llm = ChatOpenAI(model=\"gpt-5-nano\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCoFggLd1wR5"
   },
   "source": [
    "## 1. Prompt Engineering Fundamentals\n",
    "\n",
    "### Key Principles\n",
    "\n",
    "1. **Be Specific**: Clear, detailed instructions\n",
    "2. **Provide Context**: Give background information\n",
    "3. **Use Examples**: Show desired format (few-shot)\n",
    "4. **Set Constraints**: Define length, format, tone\n",
    "5. **Iterate**: Test and refine prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33072,
     "status": "ok",
     "timestamp": 1761237439301,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "DK-bWTyP1wR6",
    "outputId": "6d6d1548-1062-497f-f07d-79c66e13931e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad Prompt Response:\n",
      "AI, or artificial intelligence, is a field in computer science focused on building systems that can perform tasks that usually require human intelligence. These tasks include understanding language, recognizing images, solving problems, learning from data, and making decisions.\n",
      "\n",
      "Key ideas and distinctions\n",
      "- Narrow AI vs. general AI: Narrow (or weak) AI is designed for specific tasks (e.g., voice assistants, image classifiers, recommendation systems). General AI would be capable of any intellectual task a human can do, and is not yet reachable. Superintelligent AI, a hypothetical future stage, would surpass human intelligence across almost all areas.\n",
      "- Data-driven learning: Modern AI mostly uses data and statistical methods to learn patterns. The model improves by optimizing a mathematical objective (loss function) with lots of examples.\n",
      "- Difference from traditional software: Traditional software follows explicit rules written by humans. AI, especially machine learning, learns rules from data and can adapt to new examples without being re-programmed.\n",
      "\n",
      "Major families and techniques\n",
      "- Machine learning (ML): Algorithms learn from data to make predictions or decisions. Subtypes include supervised learning (labeled examples), unsupervised learning (discovering structure), and reinforcement learning (learning by interacting with an environment).\n",
      "- Deep learning (DL): A subset of ML that uses multi-layer neural networks. It‚Äôs especially powerful for perceptual tasks like images, sounds, and language.\n",
      "- Natural language processing (NLP): Builds systems that understand, generate, or translate human language. Examples include chatbots, translation, and sentiment analysis.\n",
      "- Computer vision (CV): Enables machines to interpret and understand visual information from the world, such as photos and videos.\n",
      "- Reinforcement learning (RL): Agents learn by trying actions and receiving feedback in an environment, aiming to maximize some notion of cumulative reward.\n",
      "- Symbolic and hybrid AI: Earlier approaches used hand-crafted rules and logic. Today many systems blend statistical learning with symbolic reasoning for better reliability and interpretability.\n",
      "\n",
      "Foundational model ideas\n",
      "- Transformer architectures: A type of neural network that excels at processing sequences (like text) and is the backbone of many large language models (LLMs) and multi-modal models.\n",
      "- Foundation models and multi-modality: Large pre-trained models can be adapted to many tasks. They often handle multiple data types (text, images, audio) and can be fine-tuned for specific applications.\n",
      "\n",
      "How AI is built and deployed (high level)\n",
      "- Data: Collect and clean large datasets; quality and representativeness matter.\n",
      "- Model design: Choose a neural network or other algorithm suitable for the task.\n",
      "- Training: Optimize a loss function using compute resources (GPUs/TPUs) on data.\n",
      "- Evaluation: Test on held-out data and measure performance with metrics relevant to the task.\n",
      "- Deployment: Integrate into software systems, with monitoring for drift, failures, and safety.\n",
      "- Maintenance: Update models as data and requirements change; monitor for bias, privacy, and reliability issues.\n",
      "\n",
      "Applications across sectors\n",
      "- Digital assistants, search, translation, chatbots\n",
      "- Healthcare: imaging analysis, diagnostics support, drug discovery\n",
      "- Finance: fraud detection, risk assessment, algorithmic trading\n",
      "- Industry and robotics: automation, predictive maintenance, autonomous machines\n",
      "- Communications, media, and customer service\n",
      "- Science and engineering: climate modeling, materials discovery, simulations\n",
      "\n",
      "Benefits and opportunities\n",
      "- Efficiency and automation of repetitive tasks\n",
      "- Data-driven insights and better decision support\n",
      "- Personalization and improved accessibility\n",
      "- New products and services enabled by advanced analytics\n",
      "\n",
      "Risks, challenges, and ethics\n",
      "- Bias and fairness: Models can reflect or amplify biases from training data.\n",
      "- Privacy: Handling sensitive data responsibly and avoiding leakage.\n",
      "- Safety and reliability: Models can fail in unusual situations or be exploited (adversarial inputs).\n",
      "- Security and misuse: AI technologies can be used for fraud, misinformation, or cyberattacks.\n",
      "- Transparency and accountability: Difficulty interpreting why a complex model made a particular decision.\n",
      "- Environment and energy costs: Training large models requires substantial compute resources.\n",
      "- Social impact: Potential job displacement and changes in how work is organized.\n",
      "\n",
      "Regulation and governance\n",
      "- Many regions are developing or implementing AI policies focused on safety, privacy, accountability, and transparency.\n",
      "- Industry standards and ethical guidelines are evolving to address risk management, data governance, and responsible deployment.\n",
      "\n",
      "What this means for you\n",
      "- If you‚Äôre a developer, you can build intelligent features by using ML/DL libraries and pre-trained models, while paying attention to ethics, bias, and privacy.\n",
      "- If you‚Äôre a business leader, AI can drive efficiency and new capabilities, but you should plan for data quality, governance, and risk management.\n",
      "- If you‚Äôre a learner, there are many resources to start with, from introductory courses to more advanced topics in ML/DL, NLP, CV, and AI safety.\n",
      "\n",
      "Would you like to dive deeper into a specific area? For example:\n",
      "- How neural networks work at a technical level\n",
      "- The difference between supervised, unsupervised, and reinforcement learning\n",
      "- What transformers and large language models are\n",
      "- Practical AI ethics and how to assess bias in models\n",
      "- How to get started with a hands-on AI project or course\n",
      "\n",
      "==================================================\n",
      "\n",
      "Good Prompt Response:\n",
      "- Definition: Artificial intelligence is software that can learn from data and make decisions, like a tiny brain inside a computer.\n",
      "- Real-world example: Siri or Alexa answering questions and helping you set reminders.\n",
      "- Why it's useful: It saves time, helps with hard tasks, and makes apps smarter and more helpful.\n"
     ]
    }
   ],
   "source": [
    "# Bad vs Good Prompts\n",
    "\n",
    "# ‚ùå Bad: Too vague\n",
    "bad_prompt = \"Tell me about AI\"\n",
    "\n",
    "# ‚úÖ Good: Specific and structured\n",
    "good_prompt = \"\"\"\n",
    "Explain artificial intelligence to a 10-year-old.\n",
    "Include:\n",
    "- A simple definition\n",
    "- One real-world example\n",
    "- Why it's useful\n",
    "Keep your response under 100 words.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Bad Prompt Response:\")\n",
    "print(llm.invoke(bad_prompt).content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Good Prompt Response:\")\n",
    "print(llm.invoke(good_prompt).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9pcvXGH1wR7"
   },
   "source": [
    "## 2. Introduction to Prompt Templates\n",
    "\n",
    "**Prompt Templates** allow you to create reusable prompts with variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1761237439349,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "GSJwzIXe1wR8",
    "outputId": "89f5a941-184d-4859-d479-4270e3ae28ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: Explain blockchain to a teenager. Keep it simple and engaging.\n",
      "\n",
      "Prompt 2: Explain quantum computing to a business executive. Keep it simple and engaging.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Create a simple template\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"audience\"],\n",
    "    template=\"Explain {topic} to a {audience}. Keep it simple and engaging.\"\n",
    ")\n",
    "\n",
    "# Use the template\n",
    "prompt1 = template.format(topic=\"blockchain\", audience=\"teenager\")\n",
    "prompt2 = template.format(topic=\"quantum computing\", audience=\"business executive\")\n",
    "\n",
    "print(\"Prompt 1:\", prompt1)\n",
    "print(\"\\nPrompt 2:\", prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfDOC1YF1wR8"
   },
   "source": [
    "## 3. Chat Prompt Templates\n",
    "\n",
    "For chat models, use **ChatPromptTemplate** to structure system, human, and AI messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5675,
     "status": "ok",
     "timestamp": 1761237445028,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "6FIjFk5m1wR8",
    "outputId": "331e6477-e6bd-4adc-d914-bb59c58a1788"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Messages:\n",
      "system: You are a helpful data scientist who explains concepts clearly.\n",
      "\n",
      "human: Explain neural networks in 3 sentences.\n",
      "\n",
      "Response:\n",
      "A neural network is a collection of interconnected units (neurons) arranged in layers that transform input data through weighted sums and nonlinear activation functions. Each connection has a weight that the model adjusts during training, allowing it to learn complex patterns from examples. Training uses algorithms like backpropagation with gradient descent to minimize a loss function, updating weights to approximate the desired input‚Äìoutput mapping.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Create a chat prompt template\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful {role} who explains concepts clearly.\"),\n",
    "    (\"human\", \"Explain {concept} in {num_sentences} sentences.\")\n",
    "])\n",
    "\n",
    "# Format the prompt\n",
    "messages = chat_template.format_messages(\n",
    "    role=\"data scientist\",\n",
    "    concept=\"neural networks\",\n",
    "    num_sentences=\"3\"\n",
    ")\n",
    "\n",
    "print(\"Messages:\")\n",
    "for msg in messages:\n",
    "    print(f\"{msg.type}: {msg.content}\\n\")\n",
    "\n",
    "# Invoke LLM with template\n",
    "response = llm.invoke(messages)\n",
    "print(\"Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGZFW1Zi1wR8"
   },
   "source": [
    "## 4. Using LCEL (LangChain Expression Language)\n",
    "\n",
    "LCEL provides a clean syntax for chaining components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5610,
     "status": "ok",
     "timestamp": 1761237450646,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "debjJMdy1wR8",
    "outputId": "c88999ce-15ec-4f27-b917-933e77305610"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Photosynthesis is the process by which green plants, algae, and some bacteria capture light energy to convert water and carbon dioxide into glucose and oxygen. The light-dependent reactions use light to produce ATP and NADPH, which power the Calvin cycle to synthesize glucose from CO2.\n"
     ]
    }
   ],
   "source": [
    "# Create a chain using the | (pipe) operator\n",
    "chain = chat_template | llm\n",
    "\n",
    "# Invoke the chain\n",
    "response = chain.invoke({\n",
    "    \"role\": \"teacher\",\n",
    "    \"concept\": \"photosynthesis\",\n",
    "    \"num_sentences\": \"2\"\n",
    "})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCIC0mLr1wR9"
   },
   "source": [
    "## 5. Few-Shot Prompting\n",
    "\n",
    "Provide examples to guide the model's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7894,
     "status": "ok",
     "timestamp": 1761237458551,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "7WOu_nD_1wR9",
    "outputId": "9a078859-c315-414d-f0b7-64779d383108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Prompt:\n",
      "Answer the following questions in a clear, technical manner:\n",
      "\n",
      "\n",
      "Question: What is Python?\n",
      "Answer: Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
      "\n",
      "Question: What is JavaScript?\n",
      "Answer: JavaScript is a versatile programming language primarily used for creating interactive web pages.\n",
      "\n",
      "Question: What is Rust?\n",
      "Answer:\n",
      "\n",
      "==================================================\n",
      "\n",
      "Response:\n",
      "Question: What is Rust?\n",
      "Answer: Rust is a systems programming language that emphasizes performance, safety, and concurrency. It compiles to native code and uses a strict ownership and borrowing model to enforce memory safety without a garbage collector, enabling predictable performance. It offers zero-cost abstractions, a strong static type system, and modern tooling (Cargo, crates.io), making it suitable for low-level, performance-critical, and concurrent applications as well as embedded systems.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "\n",
    "# Define examples\n",
    "examples = [\n",
    "    {\n",
    "        \"question\": \"What is Python?\",\n",
    "        \"answer\": \"Python is a high-level, interpreted programming language known for its simplicity and readability.\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is JavaScript?\",\n",
    "        \"answer\": \"JavaScript is a versatile programming language primarily used for creating interactive web pages.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create example template\n",
    "example_template = PromptTemplate(\n",
    "    input_variables=[\"question\", \"answer\"],\n",
    "    template=\"Question: {question}\\nAnswer: {answer}\"\n",
    ")\n",
    "\n",
    "# Create few-shot template\n",
    "few_shot_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_template,\n",
    "    prefix=\"Answer the following questions in a clear, technical manner:\\n\",\n",
    "    suffix=\"Question: {input}\\nAnswer:\",\n",
    "    input_variables=[\"input\"]\n",
    ")\n",
    "\n",
    "# Test the few-shot prompt\n",
    "prompt = few_shot_template.format(input=\"What is Rust?\")\n",
    "print(\"Few-Shot Prompt:\")\n",
    "print(prompt)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(\"Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMcI7dFE1wR9"
   },
   "source": [
    "## 6. Output Parsers\n",
    "\n",
    "Parse LLM outputs into structured formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2164,
     "status": "ok",
     "timestamp": 1761237460719,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "1hRKf02k1wR-",
    "outputId": "e78cc5c3-bccc-44b0-9812-3705e8ea2aef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'str'>\n",
      "Result: Hello! üëã How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser, CommaSeparatedListOutputParser\n",
    "\n",
    "# String Output Parser (default)\n",
    "str_parser = StrOutputParser()\n",
    "\n",
    "# Create a chain with output parser\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "chain = template | llm | str_parser\n",
    "\n",
    "result = chain.invoke({\"input\": \"Say hello!\"})\n",
    "print(f\"Type: {type(result)}\")\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4879,
     "status": "ok",
     "timestamp": 1761237465604,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "s0y7ajwW1wR-",
    "outputId": "5ab3999c-73fa-48e3-9efb-a7bd9a157b7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'list'>\n",
      "Result: ['Python', 'JavaScript', 'Java', 'C++', 'Ruby']\n"
     ]
    }
   ],
   "source": [
    "# Comma-Separated List Parser\n",
    "list_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "list_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Generate a comma-separated list as requested.\"),\n",
    "    (\"human\", \"{input}\\n{format_instructions}\")\n",
    "])\n",
    "\n",
    "# Create chain with list parser\n",
    "list_chain = list_template | llm | list_parser\n",
    "\n",
    "result = list_chain.invoke({\n",
    "    \"input\": \"List 5 programming languages\",\n",
    "    \"format_instructions\": list_parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(f\"Type: {type(result)}\")\n",
    "print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GbK2nrdm1wR-"
   },
   "source": [
    "## 7. Structured Output with Pydantic\n",
    "\n",
    "Use Pydantic models for type-safe, validated outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5210,
     "status": "ok",
     "timestamp": 1761237470826,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "E4cdrlH71wR-",
    "outputId": "e27be507-07c5-4831-e236-c9e5a0a40b3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'dict'>\n",
      "Result: {'name': 'Alex Rivera', 'age': 29, 'occupation': 'Data Scientist'}\n",
      "Name: Alex Rivera\n",
      "Age: 29\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define a Pydantic model\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"The person's name\")\n",
    "    age: int = Field(description=\"The person's age\")\n",
    "    occupation: str = Field(description=\"The person's job\")\n",
    "\n",
    "# Create JSON parser\n",
    "parser = JsonOutputParser(pydantic_object=Person)\n",
    "\n",
    "# Create prompt with format instructions\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Generate fictional person data as JSON.\"),\n",
    "    (\"human\", \"{query}\\n{format_instructions}\")\n",
    "])\n",
    "\n",
    "chain = template | llm | parser\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"query\": \"Generate data for a data scientist\",\n",
    "    \"format_instructions\": parser.get_format_instructions()\n",
    "})\n",
    "\n",
    "print(f\"Type: {type(result)}\")\n",
    "print(f\"Result: {result}\")\n",
    "print(f\"Name: {result['name']}\")\n",
    "print(f\"Age: {result['age']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcRePAOI1wR-"
   },
   "source": [
    "## 8. Partial Templates\n",
    "\n",
    "Pre-fill some variables while leaving others dynamic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17530,
     "status": "ok",
     "timestamp": 1761237488348,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "CXTkBv2Y1wR_",
    "outputId": "0ac98e3e-65db-4fb5-ce36-fae19d8b8ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today's date is 2025-10-23. Write a brief summary about quantum computing.\n",
      "\n",
      "Response:\n",
      "Here‚Äôs a brief overview of quantum computing:\n",
      "\n",
      "- Core idea: Quantum computers use qubits that can be in superposition and become entangled, enabling computation with quantum gates that manipulate these states.\n",
      "- How it works: Quantum circuits evolve qubits with unitary operations and are read out probabilistically; interference helps amplify correct results.\n",
      "- Why it matters: For some problems, quantum algorithms can offer substantial speedups‚Äîfor example, Shor‚Äôs algorithm for factoring and Grover‚Äôs search‚Äîthough not for every task.\n",
      "- Current stage: We‚Äôre in the Noisy Intermediate-Scale Quantum (NISQ) era‚Äîdevices with tens to a few hundred qubits that are noisy and not yet fault-tolerant.\n",
      "- Hardware approaches: Leading platforms include superconducting qubits, trapped ions, and photonic qubits; several companies and research groups are advancing each path.\n",
      "- Near-term applications: Chemistry and materials modeling, optimization, and certain machine-learning tasks using near-term techniques like VQE and QAOA, often with error mitigation.\n",
      "- Challenges and outlook: Key hurdles include scaling, error correction, and building a full fault-tolerant stack; in the long run, fault-tolerant quantum computing could unlock broader, practical capabilities, while hybrid quantum‚Äìclassical workflows are expected to drive near-term impact.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_current_date():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Create template with partial variables\n",
    "template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Today's date is {date}. Write a brief summary about {topic}.\",\n",
    "    partial_variables={\"date\": get_current_date}\n",
    ")\n",
    "\n",
    "# Use the template\n",
    "prompt = template.format(topic=\"quantum computing\")\n",
    "print(prompt)\n",
    "print(\"\\nResponse:\")\n",
    "print(llm.invoke(prompt).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSFl4fDC1wR_"
   },
   "source": [
    "## 9. Prompt Composition\n",
    "\n",
    "Combine multiple prompts into complex workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1761237488378,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "tQvF44uJ1wR_",
    "outputId": "ddb3d1a5-a8c1-41fc-97ab-c8e55021c807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction to Machine Learning:\n",
      "\n",
      "- Supervised learning\n",
      "- Unsupervised learning\n",
      "- Reinforcement learning\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PipelinePromptTemplate\n",
    "\n",
    "# Define sub-prompts\n",
    "intro_template = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Introduction to {topic}:\"\n",
    ")\n",
    "\n",
    "body_template = PromptTemplate(\n",
    "    input_variables=[\"introduction\", \"details\"],\n",
    "    template=\"{introduction}\\n\\n{details}\"\n",
    ")\n",
    "\n",
    "# Compose prompts\n",
    "full_template = PromptTemplate(\n",
    "    input_variables=[\"topic\", \"details\"],\n",
    "    template=\"{topic_intro}\\n\\nKey Details:\\n{details}\"\n",
    ")\n",
    "\n",
    "# Use composition\n",
    "intro = intro_template.format(topic=\"Machine Learning\")\n",
    "final_prompt = body_template.format(\n",
    "    introduction=intro,\n",
    "    details=\"- Supervised learning\\n- Unsupervised learning\\n- Reinforcement learning\"\n",
    ")\n",
    "\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBRcag_K1wR_"
   },
   "source": [
    "## 10. Advanced: Chain-of-Thought Prompting\n",
    "\n",
    "Guide the model to break down complex problems step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8791,
     "status": "ok",
     "timestamp": 1761237497175,
     "user": {
      "displayName": "Ivan Reznikov",
      "userId": "03906939862966425294"
     },
     "user_tz": -240
    },
    "id": "HROpMvhf1wSA",
    "outputId": "3b574e72-d6c5-4ee2-b179-1a1cb230eae1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a concise step-by-step solution:\n",
      "\n",
      "- Step 1: Identify what is given: speed = 60 mph, time = 2.5 hours.\n",
      "- Step 2: Use the formula: distance = speed √ó time.\n",
      "- Step 3: Compute: 60 √ó 2.5 = 150.\n",
      "- Step 4: State the result with units: distance = 150 miles.\n",
      "- Quick check: mph √ó hours works out to miles, so the units are correct.\n",
      "\n",
      "Answer: 150 miles.\n"
     ]
    }
   ],
   "source": [
    "cot_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a logical problem solver. Break down problems step-by-step.\"),\n",
    "    (\"human\", \"\"\"\n",
    "Problem: {problem}\n",
    "\n",
    "Let's solve this step by step:\n",
    "1. First, identify what we know\n",
    "2. Then, determine what we need to find\n",
    "3. Finally, solve and verify\n",
    "\n",
    "Please provide your reasoning:\"\"\")\n",
    "])\n",
    "\n",
    "cot_chain = cot_template | llm | StrOutputParser()\n",
    "\n",
    "result = cot_chain.invoke({\n",
    "    \"problem\": \"If a train travels at 60 mph for 2.5 hours, how far does it travel?\"\n",
    "})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uETq8rGV1wSA"
   },
   "source": [
    "## 11. Prompt Best Practices Summary\n",
    "\n",
    "### ‚úÖ DO:\n",
    "- Be specific and clear\n",
    "- Provide context and examples\n",
    "- Use consistent formatting\n",
    "- Test with edge cases\n",
    "- Version control your prompts\n",
    "\n",
    "### ‚ùå DON'T:\n",
    "- Use ambiguous language\n",
    "- Mix multiple tasks in one prompt\n",
    "- Assume implicit knowledge\n",
    "- Forget to set constraints\n",
    "- Hard-code values (use templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeRHt3t81wSA"
   },
   "source": [
    "## üéØ Exercise 4: Build a Code Explainer\n",
    "\n",
    "**Task**: Create a prompt template that:\n",
    "1. Takes a code snippet and programming language\n",
    "2. Explains what the code does\n",
    "3. Identifies key concepts\n",
    "4. Returns structured output (JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jdGfJPNA1wSA"
   },
   "outputs": [],
   "source": [
    "# Define your Pydantic model\n",
    "class CodeExplanation(BaseModel):\n",
    "    summary: str = Field(description=\"One-sentence summary\")\n",
    "    explanation: str = Field(description=\"Detailed explanation\")\n",
    "    concepts: list = Field(description=\"List of key concepts\")\n",
    "    complexity: str = Field(description=\"Beginner/Intermediate/Advanced\")\n",
    "\n",
    "# TODO: Implement your code explainer chain\n",
    "def explain_code(code_snippet, language):\n",
    "    \"\"\"\n",
    "    Explain a code snippet using LLM\n",
    "\n",
    "    Args:\n",
    "        code_snippet: The code to explain\n",
    "        language: Programming language\n",
    "\n",
    "    Returns:\n",
    "        Structured explanation\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Test with example\n",
    "# code = \"\"\"\n",
    "# def fibonacci(n):\n",
    "#     if n <= 1:\n",
    "#         return n\n",
    "#     return fibonacci(n-1) + fibonacci(n-2)\n",
    "# \"\"\"\n",
    "# explain_code(code, \"Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRMfXz471wSA"
   },
   "source": [
    "## üéØ Exercise 5: Dynamic Few-Shot Learning\n",
    "\n",
    "**Task**: Create a system that:\n",
    "1. Selects relevant examples based on the query\n",
    "2. Uses semantic similarity to find best examples\n",
    "3. Dynamically constructs few-shot prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QFfMkAJe1wSA"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def dynamic_few_shot(query, example_pool, k=2):\n",
    "    \"\"\"\n",
    "    Select most relevant examples using embeddings\n",
    "\n",
    "    Args:\n",
    "        query: User's query\n",
    "        example_pool: List of example dicts with 'input' and 'output'\n",
    "        k: Number of examples to select\n",
    "\n",
    "    Returns:\n",
    "        Response using dynamically selected examples\n",
    "    \"\"\"\n",
    "    # TODO: Implement dynamic example selection\n",
    "    pass\n",
    "\n",
    "# Test data\n",
    "# examples = [\n",
    "#     {\"input\": \"Translate 'hello' to French\", \"output\": \"bonjour\"},\n",
    "#     {\"input\": \"Translate 'goodbye' to French\", \"output\": \"au revoir\"},\n",
    "#     {\"input\": \"What is 2+2?\", \"output\": \"4\"},\n",
    "#     {\"input\": \"What is 5*5?\", \"output\": \"25\"}\n",
    "# ]\n",
    "# dynamic_few_shot(\"Translate 'thank you' to French\", examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cU9rdZgz1wSA"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "‚úÖ Prompt engineering fundamentals  \n",
    "‚úÖ Creating and using prompt templates  \n",
    "‚úÖ Chat prompt templates and LCEL  \n",
    "‚úÖ Few-shot prompting techniques  \n",
    "‚úÖ Output parsers for structured data  \n",
    "‚úÖ Pydantic models for type safety  \n",
    "‚úÖ Prompt composition strategies  \n",
    "‚úÖ Chain-of-thought reasoning  \n",
    "\n",
    "**Next**: We'll explore Memory and Conversation Management!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
